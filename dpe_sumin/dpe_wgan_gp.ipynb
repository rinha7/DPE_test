{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from custom_layer import *\n",
    "from image_loader import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import cv2\n",
    "import tensorflow_addons as tfa\n",
    "from utils_from_gp import pbar\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP:\n",
    "    def __init__(self):\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 3\n",
    "        self.image_size = 512\n",
    "        self.total_images = 4500\n",
    "        self.n_critic = 5\n",
    "        self.grad_penalty_weight = 10.0\n",
    "        self.g_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.d_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.G, self.g_tl = self.build_generator(keras.Input((512,512,3),batch_size=3))\n",
    "        self.D, self.d_tl = self.build_discriminator(keras.Input((512,512,3),batch_size=3))\n",
    "    \n",
    "    # 함수형 API로 구성한 Generator의 첫번째 그\n",
    "    def generator_first(self,input_):\n",
    "        tensor_list = []\n",
    "    #     input_ = keras.layers.Input(shape=(512, 512, 3),batch_size=3) # (3,512,512,3) 크기의 Input을 생성한다.\n",
    "        tensor_list.append(input_)\n",
    "        hidden1 = conv2d_layer_same(16,3,1)(input_)\n",
    "        tensor_list.append(hidden1)\n",
    "        act1 = keras.activations.selu(hidden1)\n",
    "        tensor_list.append(act1)\n",
    "        bn1 = keras.layers.BatchNormalization()(act1)\n",
    "        tensor_list.append(bn1)\n",
    "\n",
    "        hidden2 = conv2d_layer_same(32, 5, 2)(bn1)\n",
    "        tensor_list.append(hidden2)\n",
    "        act2 = keras.activations.selu(hidden2)\n",
    "        tensor_list.append(act2)\n",
    "        bn2 = keras.layers.BatchNormalization()(act2)\n",
    "        tensor_list.append(bn2)\n",
    "\n",
    "        hidden3 = conv2d_layer_same(64, 5, 2)(bn2)\n",
    "        tensor_list.append(hidden3)\n",
    "        act3 = keras.activations.selu(hidden3)\n",
    "        tensor_list.append(act3)\n",
    "        bn3 = keras.layers.BatchNormalization()(act3)\n",
    "        tensor_list.append(bn3)\n",
    "\n",
    "        hidden4 = conv2d_layer_same(128, 5, 2)(bn3)\n",
    "        tensor_list.append(hidden4)\n",
    "        act4 = keras.activations.selu(hidden4)\n",
    "        tensor_list.append(act4)\n",
    "        bn4 = keras.layers.BatchNormalization()(act4)\n",
    "        tensor_list.append(bn4)\n",
    "\n",
    "        hidden5 = conv2d_layer_same(128, 5, 2)(bn4)\n",
    "        tensor_list.append(hidden5)\n",
    "        act5 = keras.activations.selu(hidden5)\n",
    "        tensor_list.append(act5)\n",
    "        bn5 = keras.layers.BatchNormalization()(act5)\n",
    "\n",
    "        tensor_list.append(bn5)\n",
    "        model = keras.Model(inputs=[input_],outputs=bn5)\n",
    "        return model,tensor_list\n",
    "\n",
    "    def generator_second(self,input_):\n",
    "        # 논문 코드의 U-net Generator 두번째 파트에 해당하는 부분\n",
    "        # 9개의 레이어로 구성되어 있다.\n",
    "        m1, tensor_list = self.generator_first(input_)\n",
    "        input_ = tensor_list[-1]\n",
    "\n",
    "        hidden1 = conv2d_layer_same(128,5,2)(input_)\n",
    "        tensor_list.append(hidden1)\n",
    "        act1 = keras.activations.selu(hidden1)\n",
    "        tensor_list.append(act1)\n",
    "        bn1 = keras.layers.BatchNormalization()(act1)\n",
    "        tensor_list.append(bn1)\n",
    "\n",
    "        hidden2 = conv2d_layer_same(128, 5, 2)(bn1)\n",
    "        tensor_list.append(hidden2)\n",
    "        act2 = keras.activations.selu(hidden2)\n",
    "        tensor_list.append(act2)\n",
    "        bn2 = keras.layers.BatchNormalization()(act2)\n",
    "        tensor_list.append(bn2)\n",
    "\n",
    "        hidden3 = conv2d_layer_valid(128, 8, 1)(bn2)\n",
    "        tensor_list.append(hidden3)\n",
    "        act3 = keras.activations.selu(hidden3)\n",
    "        tensor_list.append(act3)\n",
    "\n",
    "        hidden4 = conv2d_layer_valid(128, 1, 1)(act3) # global concat에 사용할 image의 global feature 추출\n",
    "        tensor_list.append(hidden4)\n",
    "\n",
    "        # model = keras.Model(inputs=input_,outputs=hidden4)\n",
    "        return tensor_list\n",
    "\n",
    "    # Unet Generator의 세 번째 레이어 그룹\n",
    "    # global concat과 그냥 concat, residual block 등 다양한 커스텀 레이어를 제작할 필요가 있다.\n",
    "    # 30개의 레이어로 구성되어 있음.\n",
    "\n",
    "    def build_generator(self,input_):\n",
    "        tensor_list = self.generator_second(input_)\n",
    "        input_ = tensor_list[15] # input index = 15\n",
    "\n",
    "        # 1번째 conv 그\n",
    "        hidden1 = conv2d_layer_same(128,3,1)(input_)\n",
    "        tensor_list.append(hidden1)\n",
    "        gc1 = exe_global_concat_layer(hidden1, tensor_list, 24)\n",
    "        tensor_list.append(gc1)\n",
    "\n",
    "        # 2번째 conv 그룹\n",
    "        hidden2 = conv2d_layer_same(128,1,1)(gc1)\n",
    "        tensor_list.append(hidden2)\n",
    "        act1 = keras.activations.selu(hidden2)\n",
    "        tensor_list.append(act1)\n",
    "        bn1 = keras.layers.BatchNormalization()(act1)\n",
    "        tensor_list.append(bn1)\n",
    "\n",
    "        # 3번째 conv 그룹\n",
    "        hidden3 = conv2d_layer_same(128,3,1)(bn1)\n",
    "        tensor_list.append(hidden3)\n",
    "        rs1 = exe_resize_layer(hidden3, 2)\n",
    "        tensor_list.append(rs1)\n",
    "        #concat_layer index 10\n",
    "        concat1 = tf.concat([rs1, tensor_list[10]],axis=-1)\n",
    "        tensor_list.append(concat1)\n",
    "        act2 = keras.activations.selu(concat1)\n",
    "        tensor_list.append(act2)\n",
    "        bn2 = keras.layers.BatchNormalization()(act2)\n",
    "        tensor_list.append(bn2)\n",
    "\n",
    "        # 4번째 conv 그룹\n",
    "        hidden4 = conv2d_layer_same(128, 3, 1)(bn2)\n",
    "        tensor_list.append(hidden4)\n",
    "        rs2 = exe_resize_layer(hidden4, 2)\n",
    "        tensor_list.append(rs2)\n",
    "        # concat_layer index 7\n",
    "        concat2 = tf.concat([rs2, tensor_list[7]], axis=-1)\n",
    "        tensor_list.append(concat2)\n",
    "        act3 = keras.activations.selu(concat2)\n",
    "        tensor_list.append(act3)\n",
    "        bn3 = keras.layers.BatchNormalization()(act3)\n",
    "        tensor_list.append(bn3)\n",
    "\n",
    "        # 5번째 Conv 그룹\n",
    "        hidden5 = conv2d_layer_same(64, 3, 1)(bn3)\n",
    "        tensor_list.append(hidden5)\n",
    "        rs3 = exe_resize_layer(hidden5, 2)\n",
    "        tensor_list.append(rs3)\n",
    "        # concat_layer index 4\n",
    "        concat3 = tf.concat([rs3, tensor_list[4]], axis=-1)\n",
    "        tensor_list.append(concat3)\n",
    "        act4 = keras.activations.selu(concat3)\n",
    "        tensor_list.append(act4)\n",
    "        bn4 = keras.layers.BatchNormalization()(act4)\n",
    "        tensor_list.append(bn4)\n",
    "\n",
    "        # 6번째 Conv 그룹\n",
    "        hidden6 = conv2d_layer_same(32, 3, 1)(bn4)\n",
    "        tensor_list.append(hidden6)\n",
    "        rs4 = exe_resize_layer(hidden6, 2)\n",
    "        tensor_list.append(rs4)\n",
    "        # concat_layer index 4\n",
    "        concat4 = tf.concat([rs4, tensor_list[1]], axis=-1)\n",
    "        tensor_list.append(concat4)\n",
    "        act5 = keras.activations.selu(concat4)\n",
    "        tensor_list.append(act5)\n",
    "        bn5 = keras.layers.BatchNormalization()(act5)\n",
    "        tensor_list.append(bn5)\n",
    "\n",
    "        # 7번째 cONV 그룹\n",
    "        hidden7 = conv2d_layer_same(16,3,1)(bn5)\n",
    "        tensor_list.append(hidden7)\n",
    "        act6 = keras.activations.selu(hidden7)\n",
    "        tensor_list.append(act6)\n",
    "        bn6 = keras.layers.BatchNormalization()(act6)\n",
    "        tensor_list.append(bn6)\n",
    "\n",
    "        # 8번째 conv 그룹\n",
    "        hidden8 = conv2d_layer_same(3,3,1)(bn6)\n",
    "        tensor_list.append(hidden8)\n",
    "        res = exe_res_layer(hidden8,tensor_list,0,[0,1,2])\n",
    "\n",
    "        model = keras.Model(inputs=tensor_list[0],outputs=res)\n",
    "        return model, tensor_list\n",
    "\n",
    "\n",
    "    def build_discriminator(self,input_):\n",
    "\n",
    "    #     input_ = keras.layers.Input(shape=(512, 512, 3),batch_size=3) # i\n",
    "        tensor_list = []\n",
    "        tensor_list.append(input_)\n",
    "\n",
    "        # hidden 1\n",
    "        hidden1 = conv2d_layer_same(16,3,1)(input_)\n",
    "        tensor_list.append(hidden1)\n",
    "        act1 = keras.layers.LeakyReLU()(hidden1)\n",
    "        tensor_list.append(act1)\n",
    "        bn1 = tfa.layers.InstanceNormalization()(act1)\n",
    "        tensor_list.append(bn1)\n",
    "\n",
    "        # hidden 2\n",
    "        hidden2 = conv2d_layer_same(32, 5, 2)(bn1)\n",
    "        tensor_list.append(hidden2)\n",
    "        act2 = keras.layers.LeakyReLU()(hidden2)\n",
    "        tensor_list.append(act2)\n",
    "        bn2 = tfa.layers.InstanceNormalization()(act2)\n",
    "        tensor_list.append(bn2)\n",
    "\n",
    "        # hidden 3\n",
    "        hidden3 = conv2d_layer_same(64, 5, 2)(bn2)\n",
    "        tensor_list.append(hidden3)\n",
    "        act3 = keras.layers.LeakyReLU()(hidden3)\n",
    "        tensor_list.append(act3)\n",
    "        bn3 = tfa.layers.InstanceNormalization()(act3)\n",
    "        tensor_list.append(bn3)\n",
    "\n",
    "        # hidden 4\n",
    "        hidden4 = conv2d_layer_same(128, 5, 2)(bn3)\n",
    "        tensor_list.append(hidden4)\n",
    "        act4 = keras.layers.LeakyReLU()(hidden4)\n",
    "        tensor_list.append(act4)\n",
    "        bn4 = tfa.layers.InstanceNormalization()(act4)\n",
    "        tensor_list.append(bn4)\n",
    "\n",
    "        # hidden 5\n",
    "        hidden5 = conv2d_layer_same(128, 5, 2)(bn4)\n",
    "        tensor_list.append(hidden5)\n",
    "        act5 = keras.layers.LeakyReLU()(hidden5)\n",
    "        tensor_list.append(act5)\n",
    "        bn5 = tfa.layers.InstanceNormalization()(act5)\n",
    "        tensor_list.append(bn5)\n",
    "\n",
    "        # hidden 6\n",
    "        hidden6 = conv2d_layer_same(128, 5, 2)(bn5)\n",
    "        tensor_list.append(hidden6)\n",
    "        act6 = keras.layers.LeakyReLU()(hidden6)\n",
    "        tensor_list.append(act6)\n",
    "        bn6 = tfa.layers.InstanceNormalization()(act6)\n",
    "        tensor_list.append(bn6)\n",
    "\n",
    "        # hidden 7\n",
    "        hidden7 = conv2d_layer_valid(1,1,16)(bn6)\n",
    "        tensor_list.append(hidden7)\n",
    "        # reduce mean layer, 짧기 때문에 직접 작성하였음\n",
    "        rml = tf.reduce_mean(hidden7, [1,2,3],keepdims=False) # 파라미터는 순서대로 tensor, axis, keepdims\n",
    "        tensor_list.append(rml)\n",
    "\n",
    "        model = keras.Model(inputs=input_, outputs=rml)\n",
    "        return model, tensor_list\n",
    "    \n",
    "    def img_L2_loss(self,img1, img2):\n",
    "        return tf.reduce_mean(tf.square(tf.subtract(img1, img2)))\n",
    "    \n",
    "    def d_loss_fn(self,f_logit, r_logit):\n",
    "        f_loss = tf.reduce_mean(f_logit)\n",
    "        r_loss = tf.reduce_mean(r_logit)\n",
    "        return f_loss - r_loss\n",
    "\n",
    "\n",
    "    def g_loss_fn(self,f_logit):\n",
    "        f_loss = -tf.reduce_mean(f_logit)\n",
    "        return f_loss\n",
    "    \n",
    "    def gradient_penalty(self, f, real, fake):\n",
    "        alpha = tf.random.uniform([self.batch_size, 1, 1, 1], 0., 1.)\n",
    "        diff = fake - real\n",
    "        inter = real + (alpha * diff)\n",
    "        with tf.GradientTape() as t:\n",
    "            t.watch(inter)\n",
    "            pred = f(inter)\n",
    "        grad = t.gradient(pred, [inter])[0]\n",
    "        slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((slopes - 1.)**2)\n",
    "        return gp\n",
    "    \n",
    "    @tf.function\n",
    "    def train_g(self,raw):\n",
    "        with tf.GradientTape() as t:\n",
    "            x_fake = self.G(raw, training=True)\n",
    "            fake_logits = self.D(x_fake, training=True)\n",
    "            loss = self.g_loss_fn(fake_logits)\n",
    "        grad = t.gradient(loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(grad, self.G.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def train_d(self, raw, clean):\n",
    "        with tf.GradientTape() as t:\n",
    "            x_fake = self.G(raw, training=True)\n",
    "            fake_logits = self.D(x_fake, training=True)\n",
    "            real_logits = self.D(clean, training=True)\n",
    "            \n",
    "            cost = self.d_loss_fn(fake_logits, real_logits)\n",
    "            gp = self.gradient_penalty(partial(self.D, training=True), clean, x_fake)\n",
    "            \n",
    "            cost += self.grad_penalty_weight * gp\n",
    "        \n",
    "        grad = t.gradient(cost, self.D.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(grad, self.D.trainable_variables))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def train(self, raw, clean):\n",
    "        g_train_loss = keras.metrics.Mean()\n",
    "        d_train_loss = keras.metrics.Mean()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            bar = pbar(self.total_images,self.batch_size,epoch,self.epochs)\n",
    "            for batch in range(0, self.total_images, self.batch_size):\n",
    "#                 raw_data = (raw[batch:batch+self.batch_size].astype(np.float32) - 127.5) / 127.5\n",
    "#                 clean_data = (clean[batch:batch+self.batch_size].astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "                raw_data = raw[batch:batch+self.batch_size].astype(np.float32)\n",
    "                clean_data = clean[batch:batch+self.batch_size].astype(np.float32)\n",
    "\n",
    "                for _ in range(self.n_critic):\n",
    "                    self.train_d(raw_data,clean_data)\n",
    "                    d_loss = self.train_d(raw_data,clean_data)\n",
    "                    d_train_loss(d_loss)\n",
    "                g_loss = self.train_g(raw_data)\n",
    "                g_train_loss(g_loss)\n",
    "                self.train_g(raw_data)\n",
    "                \n",
    "                bar.postfix['g_loss'] = f'{g_train_loss.result():6.3f}'\n",
    "                bar.postfix['d_loss'] = f'{d_train_loss.result():6.3f}'\n",
    "                bar.update(self.batch_size)\n",
    "                \n",
    "            g_train_loss.reset_states()\n",
    "            d_train_loss.reset_states()\n",
    "            \n",
    "            bar.close()\n",
    "            del bar\n",
    "        \n",
    "            if epoch % 10 == 0 and epoch != 0:\n",
    "                keras.models.save_model(self.G,\"gp_images/gp_model_epoch_%2d.h5\"%epoch)\n",
    "                \n",
    "                sample_data = raw[np.random.randint(4500,size=3)]\n",
    "    \n",
    "                samples = self.G(sample_data,training=False)\n",
    "\n",
    "                print(np.max(samples[0]),\"\\n\",np.min(samples[0]))\n",
    "#                 plt.subplot(131)\n",
    "#                 plt.imshow(tf.cast((samples[0] + 1) * 127.5,np.uint8))\n",
    "#                 plt.subplot(132)\n",
    "#                 plt.imshow(tf.cast((samples[1] + 1) * 127.5,np.uint8))\n",
    "#                 plt.subplot(133)\n",
    "#                 plt.imshow(tf.cast((samples[2] + 1) * 127.5 ,np.uint8))\n",
    "#                 plt.savefig(\"gp_images/epoch_%2d_sample.png\"%epoch)\n",
    "#                 plt.show()\n",
    "                plt.subplot(131)\n",
    "                plt.imshow(tf.cast(samples[0],np.uint8))\n",
    "                plt.subplot(132)\n",
    "                plt.imshow(tf.cast(samples[1],np.uint8))\n",
    "                plt.subplot(133)\n",
    "                plt.imshow(tf.cast(samples[2],np.uint8))\n",
    "                plt.savefig(\"gp_images/epoch_%2d_sample.png\"%epoch)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iml = Loader()\n",
    "\n",
    "raw, clean = iml.load_from_npy_old()\n",
    "\n",
    "raw = raw[:4500]\n",
    "clean = clean[:4500]\n",
    "clean_after = []\n",
    "for c in clean:\n",
    "    clean_after.append(cv2.cvtColor(c,cv2.COLOR_RGB2BGR))\n",
    "clean_after = np.array(clean_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan_gp = WGANGP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b854a778e6b471e81e00e9faadfe869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=4500.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wgan_gp.train(raw,clean_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
